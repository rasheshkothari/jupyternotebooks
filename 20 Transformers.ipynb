{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Transformers\n",
    "\n",
    "Sentence transformers require Python 3.6 or higher, PyTorch 1.6.0 or higher\n",
    "Install PyTorch and TorchVision in conda (see https://pytorch.org/) and sentence transformers from the Anaconda Command Prompt:\n",
    "\n",
    "conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch\n",
    "\n",
    "python -m pip install transformers\n",
    "\n",
    "python -m pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# Check for GPU (using PyTorch)\n",
    "# If available. tell PyTorch to use the GPU, otherwise use the CPU\n",
    "\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('stsb-roberta-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: I like Python because I can build AI applications\n",
      "Sentence 2: I like Python because I can do data analytics\n",
      "Similarity score: 0.8015284538269043\n"
     ]
    }
   ],
   "source": [
    "# Calculate semantic similarity between two sentences\n",
    "sentence1 = \"I like Python because I can build AI applications\"\n",
    "sentence2 = \"I like Python because I can do data analytics\"\n",
    "\n",
    "# Encode sentences to get their embeddings\n",
    "embedding1 = model.encode(sentence1, convert_to_tensor=True)\n",
    "embedding2 = model.encode(sentence2, convert_to_tensor=True)\n",
    "\n",
    "# Compute similarity scores of two embeddings\n",
    "cosine_score = util.pytorch_cos_sim(embedding1, embedding2)\n",
    "print(\"Sentence 1:\", sentence1)\n",
    "print(\"Sentence 2:\", sentence2)\n",
    "print(\"Similarity score:\", cosine_score.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Text Similarity (STS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I like Python because I can build AI applications\n",
      "Embedding: [-0.46271408  0.74068356 -0.26615497 ...  1.6758347  -2.6872828\n",
      " -0.21768881]\n",
      "\n",
      "Sentence: I like Python because I can do data analytics\n",
      "Embedding: [-0.3860071   0.6501613  -0.30140767 ...  1.5000768  -2.2584777\n",
      "  0.7605829 ]\n",
      "\n",
      "Sentence: The cat sits on the ground\n",
      "Embedding: [-0.23815349  0.52042085 -0.2830657  ...  0.09840204 -0.5524504\n",
      "  0.40428722]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print sentence embeddings\n",
    "sentences = [\"I like Python because I can build AI applications\",\n",
    "             \"I like Python because I can do data analytics\", \n",
    "             \"The cat sits on the ground\"]\n",
    "\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: I like Python because I can build AI applications\n",
      "Sentence 2: I like Python because I can do data analytics\n",
      "Similarity Score: 0.8015283942222595\n",
      "\n",
      "Sentence 1: I like Python because I can build AI applications\n",
      "Sentence 2: The cat walks on the sidewalk\n",
      "Similarity Score: -0.03110990673303604\n",
      "\n",
      "Sentence 1: The cat sits on the ground\n",
      "Sentence 2: I like Python because I can do data analytics\n",
      "Similarity Score: 0.11328627914190292\n",
      "\n",
      "Sentence 1: The cat sits on the ground\n",
      "Sentence 2: The cat walks on the sidewalk\n",
      "Similarity Score: 0.4038146734237671\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate semantic similarity between two lists of sentences\n",
    "sentences1 = [\"I like Python because I can build AI applications\", \n",
    "              \"The cat sits on the ground\"]   \n",
    "sentences2 = [\"I like Python because I can do data analytics\", \n",
    "              \"The cat walks on the sidewalk\"]\n",
    "\n",
    "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\n",
    "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\n",
    "\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "for i in range(len(sentences1)):\n",
    "    for j in range(len(sentences2)):\n",
    "        print(\"Sentence 1:\", sentences1[i])\n",
    "        print(\"Sentence 2:\", sentences2[j])\n",
    "        print(\"Similarity Score:\", cosine_scores[i][j].item())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: I like Javascript because I can build web applications \n",
      "\n",
      "Top 2 most similar sentences in corpus:\n",
      "I like Python because I can build AI applications (Score: 0.6696)\n",
      "I like Python because I can do data analytics (Score: 0.5455)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve Top K most similar sentences from a corpus given a sentence\n",
    "corpus = [\"I like Python because I can build AI applications\",\n",
    "          \"I like Python because I can do data analytics\",\n",
    "          \"The cat sits on the ground\",\n",
    "          \"The cat walks on the sidewalk\"]\n",
    "\n",
    "sentence = \"I like Javascript because I can build web applications\"\n",
    "\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "sentence_embedding = model.encode(sentence, convert_to_tensor=True)\n",
    "\n",
    "# top_k results to return\n",
    "top_k=2\n",
    "\n",
    "# Compute similarity scores of the sentence with the corpus\n",
    "cos_scores = util.pytorch_cos_sim(sentence_embedding, corpus_embeddings)[0]\n",
    "\n",
    "# Sort and print results in decreasing order and get the first top_k\n",
    "top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "print(\"Sentence:\", sentence, \"\\n\")\n",
    "print(\"Top\", top_k, \"most similar sentences in corpus:\")\n",
    "\n",
    "for idx in top_results[0:top_k]:\n",
    "    print(corpus[idx], \"(Score: %.4f)\" % (cos_scores[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-5 most similar pairs:\n",
      "I like Python because I can build AI applications \t I like Python because I can do data analytics \t 0.8015\n",
      "I like Python because I can do data analytics \t The cat sits on the ground \t 0.1133\n",
      "I like Python because I can build AI applications \t The cat sits on the ground \t 0.1112\n"
     ]
    }
   ],
   "source": [
    "# Retreive top k most similar sentences from a corpus of sentences\n",
    "corpus = ['A man is eating food.',\n",
    "          'A man is eating a piece of bread.',\n",
    "          'The girl is carrying a baby.',\n",
    "          'A man is riding a horse.',\n",
    "          'A woman is playing violin.',\n",
    "          'Two men pushed carts through the woods.',\n",
    "          'A man is riding a white horse on an enclosed ground.',\n",
    "          'A monkey is playing drums.',\n",
    "          'Someone in a gorilla costume is playing a set of drums.']\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "# Compute cosine similarity between all pairs\n",
    "cos_sim = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "# Add all pairs to a list with their cosine similarity score\n",
    "all_sentence_combinations = []\n",
    "for i in range(len(cos_sim)-1):\n",
    "    for j in range(i+1, len(cos_sim)):\n",
    "        all_sentence_combinations.append([cos_sim[i][j], i, j])\n",
    "\n",
    "# Sort and print list by the highest cosine similarity score\n",
    "all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print(\"Top-5 most similar pairs:\")\n",
    "for score, i, j in all_sentence_combinations[0:5]:\n",
    "    print(\"{} \\t {} \\t {:.4f}\".format(sentences[i], sentences[j], cos_sim[i][j]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pretrained sentence embedding models\n",
    "\n",
    "The following models are optimized for Semantic Textual Similarity (STS).\n",
    "\n",
    "stsb-roberta-large - STSb performance: 86.39\n",
    "stsb-roberta-base - STSb performance: 85.44\n",
    "stsb-bert-large - STSb performance: 85.29\n",
    "stsb-distilbert-base - STSb performance: 85.16\n",
    "\n",
    "The following models are recommended for various applications, including various similarity and retrieval tasks, as they were trained on millions of paraphrase examples. They are currently under development, but they outperform NLI/STSb models for many tasks.\n",
    "\n",
    "paraphrase-distilroberta-base-v1 - Trained on large scale paraphrase data.\n",
    "paraphrase-xlm-r-multilingual-v1 - Multilingual version of paraphrase-distilroberta-base-v1, trained on parallel data for 50+ languages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embedding models for search queries (information extraction)\n",
    "\n",
    "The following models are optimized for question-answer retrieval in search queries. They were trained on MSMARCO Passage Ranking, a dataset with 500k real queries from Bing search.\n",
    "\n",
    "msmarco-distilbert-base-v3: MRR@10: 33.13 on MS MARCO dev set\n",
    "msmarco-roberta-base-ance-fristp: MRR@10: 33.03 on MS MARCO dev set\n",
    "\n",
    "The following model is trained on Google’s Natural Questions dataset, a dataset with 100k real queries from Google search together with the relevant passages from Wikipedia.\n",
    "\n",
    "nq-distilbert-base-v1: MRR10: 72.36 on NQ dev set (small)\n",
    "\n",
    "In Dense Passage Retrieval (DPR) for Open-Domain Question Answering, Karpukhin et al. trained models based on Google’s Natural Questions dataset:\n",
    "\n",
    "facebook-dpr-ctx_encoder-single-nq-base\n",
    "facebook-dpr-question_encoder-single-nq-base\n",
    "\n",
    "Karpukhin et al. also trained models on the combination of Natural Questions, TriviaQA, WebQuestions, and CuratedTREC.\n",
    "\n",
    "facebook-dpr-ctx_encoder-multiset-base\n",
    "facebook-dpr-question_encoder-multiset-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "febee83be16c4d5991924fdfdd2cd05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=244721538.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity: tensor([[0.6082]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('msmarco-distilbert-base-v3')\n",
    "\n",
    "query_embedding = model.encode('How big is London')\n",
    "passage_embedding = model.encode('London has 9,787,426 inhabitants at the 2011 census')\n",
    "\n",
    "print(\"Similarity:\", util.pytorch_cos_sim(query_embedding, passage_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79afab3e9af45eab80a794689ea15bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=244717164.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity: tensor([[0.6503]])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('nq-distilbert-base-v1')\n",
    "\n",
    "query_embedding = model.encode('How many people live in London?')\n",
    "\n",
    "# The passages are encoded as [ [title1, text1], [title2, text2], ...]\n",
    "passage_embedding = model.encode([['London', 'London has 9,787,426 inhabitants at the 2011 census.']])\n",
    "\n",
    "print(\"Similarity:\", util.pytorch_cos_sim(query_embedding, passage_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-lingual models\n",
    "\n",
    "The following models generate aligned vector spaces, i.e., similar inputs in different languages are mapped close in vector space. You do not need to specify the input language. \n",
    "\n",
    "Models for Semantic Similarity: Generate semantically similar sentences within one language or across languages:\n",
    "\n",
    "distiluse-base-multilingual-cased-v1: Multilingual knowledge distilled version of multilingual Universal Sentence Encoder. Supports 15 languages: Arabic, Chinese, Dutch, English, French, German, Italian, Korean, Polish, Portuguese, Russian, Spanish, Turkish.\n",
    "\n",
    "distiluse-base-multilingual-cased-v2: Multilingual knowledge distilled version of multilingual Universal Sentence Encoder. Supports 50+ languages. However, performance on the 15 languages mentioned above are a bit lower.\n",
    "\n",
    "paraphrase-xlm-r-multilingual-v1 - Multilingual version of paraphrase-distilroberta-base-v1, trained on parallel data for 50+ languages.\n",
    "\n",
    "stsb-xlm-r-multilingual: Produces similar embeddings as the stsb-bert-base model. Trained on parallel data for 50+ languages.\n",
    "\n",
    "quora-distilbert-multilingual - Multilingual version of quora-distilbert-base. Fine-tuned with parallel data for 50+ languages.\n",
    "\n",
    "Bitext mining: Describes the process of finding translated sentence pairs in two languages. The best model for this use-case is:\n",
    "\n",
    "LaBSE: Finds translation pairs across 109 languages. Works less well for assessing the similarity of sentence pairs that are not translations of each other.\n",
    "\n",
    "For detail, see https://www.sbert.net/docs/pretrained_models.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine translation\n",
    "\n",
    "For translation between any two  languages, say English to German, we need a language model pretrained for this task. T5 is a model for English-German translation that has been trained on the massive c4 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1670c1976394efa9a892304c7658796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1199.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659820f90a644bd086fc2cd4a56564f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=891691430.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f1f85efab84227a302292e4fd11d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=791656.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440b0a6b70be4b5694f3912c5052b209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1389353.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "translation = pipeline(\"translation_en_to_de\", model=\"t5-base\", tokenizer=\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich studiere gerne Datenwissenschaft und maschinelles Lernen\n"
     ]
    }
   ],
   "source": [
    "text = \"I like to study Data Science and Machine Learning\"\n",
    "translated_text = translation(text, max_length=40)[0]['translation_text']\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Huggingface community (https://huggingface.co/models?filter=translation) has created a set of pretrained language models for machine translation between different language pairs.\n",
    "\n",
    "For example, Engligh-to-Chinese translation using HelsinkiNLPs pretrained model on Huggingface (https://huggingface.co/Helsinki-NLP/opus-mt-en-zh):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1006: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6421e7eac8c542a7af0b37c2c9d8c5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1373.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c115a368410e4c0db009e39ddcf50f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=312087009.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d656171559c041f6af488fe7c71db3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=806435.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65f58ed55194f48b85c1e4b6655e298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=804600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75cbfb17c584406879ebbf29ea0a86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1617791.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e2cf5957034214af19b33f0919b34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=44.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "我喜欢学习数据科学和机器学习\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
    "\n",
    "model = AutoModelWithLMHead.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "translation = pipeline(\"translation_en_to_zh\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"I like to study Data Science and Machine Learning\"\n",
    "translated_text = translation(text, max_length=40)[0]['translation_text']\n",
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
